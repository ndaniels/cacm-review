% v2-acmsmall-sample.tex, dated March 6 2012
% This is a sample file for ACM small trim journals
%
% Compilation using 'acmsmall.cls' - version 1.3 (March 2012), Aptara Inc.
% (c) 2010 Association for Computing Machinery (ACM)
%
% Questions/Suggestions/Feedback should be addressed to => "acmtexsupport@aptaracorp.com".
% Users can also go through the FAQs available on the journal's submission webpage.
%
% Steps to compile: latex, bibtex, latex latex
%
% For tracking purposes => this is v1.3 - March 2012

\documentclass[prodmode,acmtecs]{acmsmall} % Aptara syntax

% Package to generate and customize Algorithm as per ACM style
\usepackage[ruled]{algorithm2e}
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

% Metadata Information
% \acmVolume{9}
% \acmNumber{4}
% \acmArticle{39}
% \acmYear{2010}
% \acmMonth{3}

% Copyright
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}
%
% % DOI
% \doi{0000001.0000001}
%
% %ISSN
% \issn{1234-56789}

% Document starts
\begin{document}

% Page heads
\markboth{B. Berger et al.}{Title}

% Title portion
\title{Title}
\author{BONNIE BERGER
\affil{Massachusetts Institute of Technology}
\author{NOAH M. DANIELS
\affil{Massachusetts Institute of Technology}
\author{Y. WILLIAM YU
\affil{Massachusetts Institute of Technology}
% NOTE! Affiliations placed here should be for the institution where the
%       BULK of the research was done. If the author has gone to a new
%       institution, before publication, the (above) affiliation should NOT be changed.
%       The authors 'current' address may be given in the "Author's addresses:" block (below).
%       So for example, Mr. Abdelzaher, the bulk of the research was done at UIUC, and he is
%       currently affiliated with NASA.

\begin{abstract}
abstract goes here
\end{abstract}


%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
% \begin{CCSXML}
% <ccs2012>
%  <concept>
%   <concept_id>10010520.10010553.10010562</concept_id>
%   <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%   <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10010520.10010575.10010755</concept_id>
%   <concept_desc>Computer systems organization~Redundancy</concept_desc>
%   <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10010520.10010553.10010554</concept_id>
%   <concept_desc>Computer systems organization~Robotics</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10003033.10003083.10003095</concept_id>
%   <concept_desc>Networks~Network reliability</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
% </ccs2012>
% \end{CCSXML}
%
% \ccsdesc[500]{Computer systems organization~Embedded systems}
% \ccsdesc[300]{Computer systems organization~Redundancy}
% \ccsdesc{Computer systems organization~Robotics}
% \ccsdesc[100]{Networks~Network reliability}

%
% End generated code
%

% We no longer use \terms command
%\terms{Design, Algorithms, Performance}

\keywords{computational biology, database search, metagenomics, chemogenomics}

\acmformat{Bonnie Berger, Noah M. Daniels,
and Y. William Yu, 2015. Title.}
% At a minimum you need to supply the author names, year and a title.
% IMPORTANT:
% Full first names whenever they are known, surname last, followed by a period.
% In the case of two authors, 'and' is placed between them.
% In the case of three or more authors, the serial comma is used, that is, all author names
% except the last one but including the penultimate author's name are followed by a comma,
% and then 'and' is placed before the final author's name.
% If only first and middle initials are known, then each initial
% is followed by a period and they are separated by a space.
% The remaining information (journal title, volume, article number, date, etc.) is 'auto-generated'.

\begin{bottomstuff}
% This work is supported by the National Science Foundation, under
% grant CNS-0435060, grant CCR-0325197 and grant EN-CS-0329609.
%
% Author's addresses: G. Zhou, Computer Science Department,
% College of William and Mary; Y. Wu  {and} J. A. Stankovic,
% Computer Science Department, University of Virginia; T. Yan,
% Eaton Innovation Center; T. He, Computer Science Department,
% University of Minnesota; C. Huang, Google; T. F. Abdelzaher,
% (Current address) NASA Ames Research Center, Moffett Field, California 94035.
\end{bottomstuff}

\maketitle


\section{Introduction}

Computational biologists answer biological and biomedical 
questions by using computation in support of---or in place of---laboratory 
procedures, in hopes of obtaining more accurate answers at a greatly reduced 
cost.
The past two decades have seen unprecedented technological progress with regard
to generating biological data; next-generation sequencing, mass spectroscopy,
microarrays, nuclear magnetic resonance, and other high-throughput approaches
have led to an explosion of data.
This explosion of data is a mixed blessing.
On the one hand, the scale and scope of data should allow new insights into
genetic and infections diseases, cancer, basic biology, and even human migration
patterns.
On the other hand, researchers are generating datasets so massive that it has 
become difficult to analyze them to discover patterns that give clues to the 
underlying biological processes.

Certainly, computers are getting faster and more economical; the amount of 
processing available per dollar of compute hardware is more or less doubling 
every year or two; a similar claim can be made about storage capacity 
(Figure~\ref{moore}).
In 2002, when the first human genome was sequenced, the growth in computing 
power was still matching the growth rate of genomic data.
However, the sequencing technology used for the Human Genome Project--Sanger
sequencing--was supplanted around 2004, with the advent of what is now known as 
next-generation sequencing.
The material costs to sequence a genome have plummeted in the past
decade, to the point where a whole human genome can be sequenced for less than
one thousand U.S. Dollars.
As a result, by some estimates, the amount of genomic data available to 
researchers is increasing by a factor of 10 every year.

This growth in data poses significant challenges for 
researchers~\cite{marx2013biology}.
One approach to solving these challenges is to embrace cloud computing.
Google, Inc. and the Broad Institute have teamed up to bring the GATK (Genome 
Analysis Toolkit) to the Google cloud (https://cloud.google.com/genomics/gatk).
ALSO MENTION AMAZON AWS HERE.
However, while cloud computing frees researchers from maintaining their own
data centers, and provides cost-saving benefits when computing resources are
not needed around the clock, it is no panacea.
First, in the face of disease outbreaks such as the 2014 Ebola virus epidemic 
in West Africa, analysis resources are needed at often-remote field sites.
While it is now possible to bring sequencing equipment and limited computing 
resources to remote sites, internet connectivity is still highly constrained.
Thus, accessing cloud resources for analytics may not be possible.
Moreover, cloud computing does not truly address the problem posed by
a corpus of data that is growing faster than Moore's law, because the computer
systems that make up those cloud datacenters are still themselves bound by
improvements in semiconductor technology; they still follow the trajectory
suggested by Moore's law.



cite nature article (Marks)
mention cloud computing, but it doesn't truly address the problem
algorithms for coming up with novel biological insights NOT focus of this review
storage requirements, network transmission, etc.

[image from: http://www.nature.com/nature/journal/v498/n7453/full/498255a.html; picture from EBI]

This Nature article pointed out that currently many omics applications require us to store, access and analyze large libraries of data.
However, the field is realizing that we don’t have the computational tools to tackle these tasks, as Arend Sidow noted.


It’s tempting to think that cloud computing is going to solve this problem.
But that’s not the case.
It might save a little money but doesn’t address the fundamental issue.
It doesn’t change the problem that the data is increasing exponentially faster than computing power per dollar.

Furthermore, as transmission bottlenecks grow as an issue, even getting data into the cloud presents a challenge.

The only solution is to discover fundamentally better algorithms for processing these databases.
Better algorithms can make an enormous difference.

In fact, we need [click] algorithms…
And that’s what we do.
We design algs that do these calculations a lot faster and space-efficiently, and thus their cost doesn’t explode as the size of the databases increase.
* In this talk, we will focus on sublinear-time algorithms, but we also get sublinear-space.



dim reduction - PCA, DCA
indexing - FM-index
kmer approaches (Kingsford)
compressed sensing (Linial)
low-dim polytopes (Alon)
tree decomposition
isorank

Make use of a completely different paradigm for the structure of biological data. By combining redundancy with a low fractal dimension, able to develop algorithms that scale.

Next-gen sequencing, and some applications
Slide 27 to illustrate how sequencing works (2a, 2b)
Slide 28: sequence analysis pipeline (2c)
Redundancy - 
- 1000 genomes project, flybase, wormbase
- subsequence similarity (k-mer approaches)



Exploiting novel structure in biological data
- What data looks like
- actual fractal dimension & metric entropy of example data sets: table
 - metagenomics
 - protein seq
 - bacterial genomes
 - protein structure
 - chemogenomics
- Runtime
- Storage
- Search as exemplar.

Sequence-based data
- cite NRG, see cablast
- cora
- masai, gem
- (include stuff about novelty in mapping over k-mer based approaches from Cora letter to editor)
- existing mappers, cite NRG paper, and pick up where it leaves off
NOT talking about assembly
- metagenomics

Beyond sequence data
- chemogenomics
- protein structure data

Conclusions and future prospect

Other methods from image processing, etc.
Beyond biological data


for other approaches that have NOT been used in biology, see discussion
in conclusion:
other approaches that haven't been used in comp bio: 
- computational geometry (Piotr Indyk)
- succinct data structures
- 






sequence approaches

structure approaches

abstraction to distance functions

no coordinate system






these observations about bio data will also hold for data from wildly different sources.

be sure to cite NRG paper, for how to extract structure from high-dimensional data.

% Figure~\ref{fig:one} shows a typical microcontroller C program that
% controls an automotive power window lift. The program is one of the
% programs used in the case study described in Section~\ref{sec:sim}.
% At first sight, the programs looks like an ANSI~C program. It
% contains function calls, assignments, if clauses, and while loops.
% Figure
% \begin{figure}
% \centerline{\includegraphics{acmsmall-mouse}}
% \caption{Code before preprocessing.}
% \label{fig:one}
% \end{figure}




% Bibliography
\bibliographystyle{ACM-Reference-Format-Journals}
\bibliography{main}
                             % Sample .bib file with references that match those in
                             % the 'Specifications Document (V1.5)' as well containing
                             % 'legacy' bibs and bibs with 'alternate codings'.
                             % Gerry Murray - March 2012


\end{document}
% End of v2-acmsmall-sample.tex (March 2012) - Gerry Murray, ACM


