% v2-acmsmall-sample.tex, dated March 6 2012
% This is a sample file for ACM small trim journals
%
% Compilation using 'acmsmall.cls' - version 1.3 (March 2012), Aptara Inc.
% (c) 2010 Association for Computing Machinery (ACM)
%
% Questions/Suggestions/Feedback should be addressed to => "acmtexsupport@aptaracorp.com".
% Users can also go through the FAQs available on the journal's submission webpage.
%
% Steps to compile: latex, bibtex, latex latex
%
% For tracking purposes => this is v1.3 - March 2012

\documentclass[prodmode,acmtecs]{acmsmall} % Aptara syntax

\usepackage{caption}
\usepackage{subfig}

% Metadata Information
% \acmVolume{9}
% \acmNumber{4}
% \acmArticle{39}
% \acmYear{2010}
% \acmMonth{3}

% Copyright
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}
%
% % DOI
% \doi{0000001.0000001}
%
% %ISSN
% \issn{1234-56789}

% Document starts
\begin{document}

% Page heads
\markboth{B. Berger et al.}{Title}

% Title portion
\title{Title}
\author{BONNIE BERGER
\affil{Massachusetts Institute of Technology}
NOAH M. DANIELS
\affil{Massachusetts Institute of Technology}
Y. WILLIAM YU
\affil{Massachusetts Institute of Technology}}


\begin{abstract}
abstract goes here
\end{abstract}


%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
% \begin{CCSXML}
% <ccs2012>
%  <concept>
%   <concept_id>10010520.10010553.10010562</concept_id>
%   <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%   <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10010520.10010575.10010755</concept_id>
%   <concept_desc>Computer systems organization~Redundancy</concept_desc>
%   <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10010520.10010553.10010554</concept_id>
%   <concept_desc>Computer systems organization~Robotics</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10003033.10003083.10003095</concept_id>
%   <concept_desc>Networks~Network reliability</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
% </ccs2012>
% \end{CCSXML}
%
% \ccsdesc[500]{Computer systems organization~Embedded systems}
% \ccsdesc[300]{Computer systems organization~Redundancy}
% \ccsdesc{Computer systems organization~Robotics}
% \ccsdesc[100]{Networks~Network reliability}

%
% End generated code
%

% We no longer use \terms command
%\terms{Design, Algorithms, Performance}

\keywords{computational biology, database search, metagenomics, chemogenomics}

\acmformat{Bonnie Berger, Noah M. Daniels,
and Y. William Yu, 2015. Title.}
% At a minimum you need to supply the author names, year and a title.
% IMPORTANT:
% Full first names whenever they are known, surname last, followed by a period.
% In the case of two authors, 'and' is placed between them.
% In the case of three or more authors, the serial comma is used, that is, all author names
% except the last one but including the penultimate author's name are followed by a comma,
% and then 'and' is placed before the final author's name.
% If only first and middle initials are known, then each initial
% is followed by a period and they are separated by a space.
% The remaining information (journal title, volume, article number, date, etc.) is 'auto-generated'.

\begin{bottomstuff}
% This work is supported by the National Science Foundation, under
% grant CNS-0435060, grant CCR-0325197 and grant EN-CS-0329609.
%
% Author's addresses: G. Zhou, Computer Science Department,
% College of William and Mary; Y. Wu  {and} J. A. Stankovic,
% Computer Science Department, University of Virginia; T. Yan,
% Eaton Innovation Center; T. He, Computer Science Department,
% University of Minnesota; C. Huang, Google; T. F. Abdelzaher,
% (Current address) NASA Ames Research Center, Moffett Field, California 94035.
\end{bottomstuff}

\maketitle


\section{Introduction}

Computational biologists answer biological and biomedical 
questions by using computation in support of---or in place of---laboratory 
procedures, in hopes of obtaining more accurate answers at a greatly reduced 
cost.
The past two decades have seen unprecedented technological progress with regard
to generating biological data; next-generation sequencing, mass spectroscopy,
microarrays, nuclear magnetic resonance, and other high-throughput approaches
have led to an explosion of data.
This explosion of data is a mixed blessing.
On the one hand, the scale and scope of data should allow new insights into
genetic and infections diseases, cancer, basic biology, and even human migration
patterns.
On the other hand, researchers are generating datasets so massive that it has 
become difficult to analyze them to discover patterns that give clues to the 
underlying biological processes.

Certainly, computers are getting faster and more economical; the amount of 
processing available per dollar of compute hardware is more or less doubling 
every year or two; a similar claim can be made about storage capacity 
(Figure~\ref{fig:growth}).
In 2002, when the first human genome was sequenced, the growth in computing 
power was still matching the growth rate of genomic data.
However, the sequencing technology used for the Human Genome Project--Sanger
sequencing--was supplanted around 2004, with the advent of what is now known as 
next-generation sequencing.
The material costs to sequence a genome have plummeted in the past
decade, to the point where a whole human genome can be sequenced for less than
one thousand U.S. Dollars.
As a result, by some estimates, the amount of genomic data available to 
researchers is increasing by a factor of 10 every year.

\begin{figure}[htb!]
\centering
\subfloat[Growth of genomic sequence data as compared with the combined power of the top-500 supercomputer list; y-axis is logarithmic]{
\includegraphics[width=3.0in]{assets/moore.png}
\label{fig:moore}
}\\
\subfloat[Growth of genomic sequence data as compared with hard drive capacity; y-axis is logarithmic]{
\includegraphics[width=3.0in]{assets/kryder.png}
\label{fig:kryder}
}
\caption[Sequencing vs. Storage]{Moore's and Kryder's laws contrasted with genomic sequence data}
\label{fig:growth}
\end{figure}

This growth in data poses significant challenges for 
researchers~\cite{marx2013biology}.
Currently, many omics applications require us to store, access, and analyze
large libraries of data.
One approach to solving these challenges is to embrace cloud computing.
Google, Inc. and the Broad Institute have teamed up to bring the GATK (Genome 
Analysis Toolkit) to the Google cloud (https://cloud.google.com/genomics/gatk).
Amazon Web Services are also commonly used for computational biology 
research~\cite{schatz2010cloud}.
However, while cloud computing frees researchers from maintaining their own
data centers, and provides cost-saving benefits when computing resources are
not needed around the clock, it is no panacea.
First, in the face of disease outbreaks such as the 2014 Ebola virus epidemic 
in West Africa, analysis resources are needed at often-remote field sites.
While it is now possible to bring sequencing equipment and limited computing 
resources to remote sites, internet connectivity is still highly constrained.
Thus, accessing cloud resources for analytics may not be possible.
Moreover, cloud computing does not truly address the problem posed by
a corpus of data that is growing faster than Moore's law, because the computer
systems that make up those cloud datacenters are still themselves bound by
improvements in semiconductor technology; they still follow the trajectory
suggested by Moore's law.

Computer scientists routinely exploit peculiarities in the structure of data in
order to reduce time or space complexity.
In computational biology, this approach has served researchers well.
Now-classical approaches such as principal component analysis (PCA) reduce the 
dimensionality of data in order to simplify analysis and uncover salient 
features.
This review focuses on algorithmic advances for dealing with the growth in biological data; algorithms for novel biological insights are not its focus.

\section{Biological sequence data and next-generation sequencing}

Many, but not all, of the problems in computational biology deal with sequence
data, either nucleotide sequences (nominally a four-letter alphabet) or protein
sequences (nominally a twenty-letter alphabet).
Certainly, as sequencing is generating the greatest volume of biological data,
a large portion of bioinformatics algorithms deal primarily with string data.
For the problem of exact search, approaches for transforming and indexing 
string data provide benefits.
The Burrows-Wheeler transform (BWT)~\cite{burrows1994block} provides efficient string 
compression through a reversible transformation, while the FM-index data 
structure~\cite{ferragina2000opportunistic} is a compressed substring index, based
on the BWT, which provides efficient storage as well as fast search.
Modern next-generation sequencing (NGS) technologies read short fragments of
DNA, known as \emph{reads}~\ref{fig:shotgun}, which must be mapped onto a reference sequence~\ref{fig:ngs-pipeline}
BWA (Burrows-Wheeler Aligner)~\cite{li2009fast} uses the BWT, while the 
Bowtie2~\cite{langmead2012fast} aligner further relies on the FM-index for efficient 
mapping of NGS reads.



\begin{figure}[htb!]
\centering
\subfloat[``Shotgun'' sequencing breaks DNA molecules into many short fragments, or reads, and relies on high coverage to produce a statistically likely representation of a whole genome]{
\includegraphics[width=3.0in]{assets/shotgun.png}
\label{fig:shotgun}
}
\subfloat[Single-nucleotide polymorphisms, or SNPs, are the simplest type of genomic variant, and form the bulk of ``variant-calling'' analysis]{
\includegraphics[width=3.0in]{assets/snps.png}
\label{fig:snps}
}\\
\subfloat[The NGS downstream analysis pipeline. Shotgun reads are mapped to a reference genome with tools such as BWA or Bowtie. The resulting genomic sequence is analyzed for variants with tools such as GATK or Samtools. This allows relationships between genes and diseases to be uncovered.]{
\includegraphics[width=3.0in]{assets/ngs-pipeline.png}
\label{fig:ngs-pipeline}
}
\caption[The NGS Pipeline]{The next-generation sequencing (NGS) pipeline.}
\label{fig:ngs}
\end{figure}

As biological sequence data is string data, it is frequently useful to index or
count instances of $k$-mers of a fixed length, $k$.
Search tools such as BLAST~\cite{altschul1990basic} rely on $k$-mer \emph{seeds} in a 
seed-and-extend approach to sequence search and alignment.
For counting $k$-mer frequencies, Jellyfish~\cite{marccais2011fast} relies on an efficient
hash-table encoding as well as Intel's ``compare-and-swap'' CPU instructions for
improved run-time performance.

Seeking to take advantage of sequence redundancy, \cite{loh2012compressive}
introduced \emph{compressive genomics}, an approach that relies on compressing
data in such a way that the desired computation (such as BLAST search) can be
performed in the compressed representation.
This approach relies on a two-stage search, referred to as \emph{coarse} and \emph{fine} search.
Coarse search is performed only on the coarse, or representative, subsequences
that represent unique data.
Any coarse sequence that is within some threshold of the query is then expanded 
into all the similar sequences it represents; the fine search is then over this
(typically small) subset of the original database.
This approach provides run-time improvements to BLAST search on 
nucleotide~\cite{loh2012compressive} and protein~\cite{daniels2013compressive}
data; moreover, these run-time improvements increase as databases grow.
Also taking advantage of sequence redundancy, Mince~\cite{patro2015data} groups 
similar reads (those that share a common 
substring) together into ``buckets'', allowing that common substring to now be 
removed and treated as the bucket label, so that each read in the compressed 
representation comprises only its unique differences from the bucket label.
This allows a general-purpose compressor to achieve tight compression.
SCALCE~\cite{hach2012scalce} also relies on a ``boosting'' scheme, reordering
reads in such a way that a general-purpose compressor achieves improved
compression ratios.

Along with a genome sequence, modern sequencing systems provide a measure of
the quality of, or confidence in, the nucleotide base read at every position.
While nucleotides reside in a 4-letter alphabet, na\"ively representable in two 
bits per position, these quality scores occupy a larger range, na\"ively
requiring six or seven bits per position.
Thus, quality scores offer low-hanging fruit with respect to compression for 
storage and transmission of sequence data.
Several approaches to compressing quality scores have been 
presented~\cite{janin2013adaptive,bonfield2013compression,ochoa2013qualcomp,yu2015quality}.
Quartz~\cite{yu2015quality} takes the unusual approach of applying lossy 
compression, by bounding the likelihood that a quality score is informative.
Surprisingly, Quartz not only demonstrates improved compression over competing
approaches, but slightly improves the accuracy of downstream variant-calling.

The idea of taking advantage of redundancy in sequence data is particularly
important in the face of new sources of data.
Driven by the plummeting costs of next-generation sequencing, the 1000 Genomes Project~\cite{10002012integrated} is pursuing a broad catalog of human 
variation; instead of producing a single reference genome for a species, many
complete genomes are catalogued.
Likewise, WormBase~\cite{harris2014wormbase} and 
FlyBase~\cite{tweedie2009flybase} are cataloguing many different species and
strains of the \emph{Caenorhabditis} worm and \emph{Drosophila} fruit fly, 
respectively.
On a potentially even larger scale is the growth of metagenomic data.
Metagenomics is the study of the many genomes (bacterial, fungal, and even 
viral) that make up a particular environment.
Such an environment could be soil from a particular region (which can lead to 
the discovery of new 
antibiotics~\cite{gillespie2002isolation,forsberg2012shared}), or it could be 
the human gut, whose microbiome has been linked to human-health concerns 
including Autism Spectrum Disorder~\cite{macfabe2012short}, 
Crohn's Disease~\cite{gevers2014treatment,manichanh2006reduced}, and 
obesity~\cite{greenblum2012metagenomic}.

The continued ability to search and analyze these growing data sets hinges on
clever algorithms that take advantage of the structure of, and redundancy 
present in, the data.
Indeed, these growing data sets ``threaten to make the arising problems 
computationally infeasible.''~\cite{berger2013computational}

One approach to more quickly searching these data sets is to come up with an
indexing technique that leverages domain knowledge about the structure of the
data.
In the realm of NGS read mapping, the Genome Multitool (GEM)
mapper~\cite{marco2012gem} makes use of a Ferragina-Manzini 
index~\cite{ferragina2000opportunistic} coupled with dynamic programming in
a compressed representation, in order to prune the search space when mapping
reads to a reference genome.
Masai~\cite{siragusa2013fast} uses an ``approximate seed'' approach to indexing
the space of possible matches, likewise pruning the search space.

Another approach is to make use of a completely different paradigm for the 
structure of biological data. 
By combining redundancy with a low fractal 
dimension~\cite{yu2015entropy}, it may be possible to develop algorithms that 
scale even as the data continue to grow.
The CORA read mapper~\cite{yorukoglu2015compressive} applies a $k$-mer based 
read-compression approach with a compressive indexing of the reference genome
(referred to as a homology table).
Short seed-clustering schemes, such as those used in Masai and 
MrsFAST~\cite{hach2010mrsfast} conceptually differ from CORA in that those 
schemes aim to accelerate only the seed-to-reference matching step.
Thus, there is a subsequent seed-extension step, which is substantially more 
costly and still needs to be performed for each read and mapping individually, 
even when seeds are clustered. 
Through its k-mer based read compression model, CORA is able to accelerate and 
achieve asymptotically sublinear scaling for both the seed-matching and 
seed-extension steps within coarse-mapping, which comprises the major bulk of 
the read-mapping computation.

In the realm of metagenomic search, BLASTX~\cite{altschul1990basic} is commonly
used to translate nucleotide reads into their possible protein sequences, and
search for them in a protein database.
Recent advances in metagenomic search tools have relied on three advances over
BLASTX: indexing, alphabet reduction, and compression.
RapSearch2~\cite{zhao2012rapsearch2} relies on alphabet reduction and a 
collision-free hash table.
The alphabet reduction, as it is reversible, can be thought of as a form of
lossless compression; a 20-letter amino acid alphabet is mapped onto a smaller
alphabet, with offsets stored to recover the original sequence in the full
alphabet.
The hash-table provides an efficient index of the database to be searched.
DIAMOND~\cite{buchfink2014fast} also relies on alphabet reduction, but uses
``shaped seeds'' instead of simple $k$-mer seeds to index the database.

The recently-released MICA~\cite{yu2015entropy} demonstrates that the 
compressive-acceleration approach of 
caBLAST~\cite{loh2012compressive} and caBLASTP~\cite{daniels2013compressive} is 
largely orthogonal to alphabet-reduction and indexing approaches.
MICA applies the compressive-acceleration framework to DIAMOND, using DIAMOND
for its ``coarse search'' phase and a user's choice of DIAMOND or BLASTX for its
``fine search'' phase; it demonstrates comparable run-time gains over DIAMOND as
caBLASTP does over BLASTP.

\section{Non-sequence data}

Another important type of biological data is gene expression, or the quantity of
a particular gene product (usually a protein) present in the cell.
Gene expression data is useful for relating genotype to phenotype, and is 
important in studying diseases including cancer.
Usually obtained through microarrays or RNA-seq technologies, expression data is
quantitative, as each gene from a sample is associated with a numeric expression
level, and high-dimensional, as many thousands of genes may be analyzed at a
time.
While expression data lends itself to cluster analysis and probabilistic 
approaches, the high dimensionality presents challenges~\cite{nguyen2002tumor}.
Principal Component Analysis has shown promise in reducing the dimensionality
of gene expression data~\cite{raychaudhuri2000principal,alter2000singular,yeung2001principal,misra2002interactive}.
Some recent work has explored other ways to exploit the high-dimensional 
structure of the data.
SPARCLE (SPArse ReCovery of Linear combinations of Expression)~\cite{prat2011recovering}
brings ideas from compressed sensing~\cite{candes2005decoding,candes2006stable,donoho2006compressed} 
to gene expression analysis.
Specifically, SPARCLE relies on the idea that a randomly-chosen set of points 
are unlikely to be close in a lower-dimensional linear subspace.
Thus, SPARCLE approximates a gene's expression profile as a linear combination
of a few other genes' profiles, and then applies supervised learning to
predict relationships between genes.

Another recent and novel approach to exploiting the structure of gene expression
space is Parti (Pareto task inference)~\cite{hart2015inferring}, which describes a set of
data as a polytope, and infers the specific tasks represented by vertices of
that polytope from the features most highly enriched at those vertices.
In human breast tumors, and in mouse tissues, the expression data were
well-described by tetrahedra whose vertices were enriched for different tumor
types and biological functions, from which~\cite{hart2015inferring} infer four distinct
tasks.

Networks are frequently used to represent biological data, such as the genetic
and physical interactions among proteins, such as those in metabolic pathways.
Several approaches have taken advantage of the particular topologies of these
networks, such as diffusion-based approaches, which explore the topology of
networks through random walks.
IsoRank~\cite{singh2008global} and IsoRankN~\cite{liao2009isorankn} perform global multiple network 
alignment based on network topology and conserved biological function.
IsoRank's approach is similar to the PageRank algorithm~\cite{page1999pagerank}, while
IsoRankN produces a \emph{Personalized PageRank} alignment, similar to the
PageRank-Nibble algorithm~\cite{andersen2006local}.
In protein interaction networks, some nodes are high-degree \emph{hubs,} which 
interact with
many partners; these interactions do not imply shared function as strongly as
non-hub interactions.
Diffusion state distance (DSD)~\cite{cao2013going} is another diffusion-based approach
which defines a distance metric based on random walks.
DSD penalizes high-degree nodes, and outperforms other graph-theoretic distance
functions when applied to standard approaches such as neighbor voting for function annotation.

As new functional annotations and protein-protein interactions are discovered,
the number and size of these networks grows.
Diffusion component analysis (DCA)~\cite{cho2015diffusion} amounts to a network-based 
analogue of principle component analysis; DCA reduces the dimensionality of a
network in such a way that function annotation can be performed in the reduced 
space, significantly reducing running time while still providing competitive
prediction accuracy.

In the realm of protein structure prediction, it has been 
observed~\cite{dunbrack2002rotamer} that rather than exploring the continuous 
range of
possible dihedral angles, protein structures inhabit a smaller subspace, defined
by a narrower range of angles, referred to as 
rotamers~\cite{dunbrack1993backbone}.
The TreePack algorithm~\cite{xu2006fast} takes advantage of this observation 
for the
purpose of sidechain packing (finding the lowest-energy conformation for the
sidechain of each amino acid), using tree width to reduce the search space and
simplify dependencies.

The compressive acceleration approach of \cite{loh2012compressive} has been
generalized and adapted to non-sequence spaces as well.
Chemogenomics, the computational study of small-molecule compounds and their
affect on phenotype, relies on comparing chemical graph structures to identify
similar molecules and binding sites.
However, there are once again an increasing number of such chemical compounds
to search; the NCBI's PubChem database has grown from 31 million compounds in
January 2011 to 68 million in July, 2015.
Furthermore, comparing chemical graph structures typically involves computing
the maximal common subgraph (MCS), an NP-hard problem.
One approach to this type of search is the Small Molecule Subgraph Detector 
(SMSD)~\cite{rahman2009small}, which applies one of several MCS algorithms based
on the size and complexity of the graphs in question.
Applying a compressive acceleration approach, Ammolite~\cite{yu2015entropy}
accelerates SMSD search by an average of 150x on the PubChem database.

When the structure of a protein is known but its biological function or 
evolutionary relationships are not, researchers may search for structurally
similar proteins that are better studied~\cite{gibrat1996surprising}.
Classical tools for this involve performing pairwise structural alignments to
look for geometric similarity; DALI~\cite{holm1995dali} is still widely used,
along with other aligners such as FATCAT~\cite{ye2004fatcat} and 
Matt~\cite{menke2008matt}.
FragBag~\cite{budowski2010fragbag} accelerates protein structure search by
approximating structural alignments, which can be computationally demanding,
by instead comparing the ``bag-of-words'' from each structure.
Analogous to a term-frequency vector in information retrieval, this bag-of-words
indicates the abundance of particular, short structural motifs within a protein.
Again applying a compressive acceleration approach, 
esFragBag~\cite{yu2015entropy} clusters proteins based on the cosine distance or
Euclidean distance of their bag-of-words vectors, further accelerating FragBag's
running time by an average of 10x.

\section{Conclusions and future prospects}

The explosion of biological data, largely due to technological advances such as
next-generation sequencing, presents us with challenges as well as 
opportunities.
The promise of unlocking the secrets of diseases such as cancer, obesity,
Alzheimer's, autism spectrum disorder, and many others, as well as better 
understanding the basic science of biology, relies on researchers' ability to 
analyze the growing flood of genomic, metagenomic, structural, and interactome 
data.

The approach of compressive acceleration~\cite{loh2012compressive}, and its 
demonstrated ability to scale with the metric entropy of the 
data~\cite{yu2015entropy}, while providing orthogonal benefits to many other
useful indexing techniques, is an important tool for coping with the deluge of
data.
The extension of this compressive acceleration approach to 
metagenomics~\cite{yu2015entropy}, NGS read 
mapping and chemogenomics~\cite{yorukoglu2015compressive} suggests its 
flexibility.

The field of computational biology must continue to innovate, but also to 
incorporate the best ideas from other areas of computer science.
For example, the compressive-acceleration approach bears similarity to a metric 
ball tree, first described in the database community over twenty years 
ago~\cite{uhlmann1991satisfying}.
Other ideas from image processing, computational geometry, and other areas 
outside of biology are likely to bear fruit.
It is also likely that algorithmic ideas developed within computational biology
will become useful in other fields experiencing a data deluge, such as astronomy
or social networks~\cite{stephens2015big}.

While this Review has focused on the importance of algorithms that leverage the
structure of data to cope with the increased throughput in computational 
biology, it is also essential that techniques continue to make better 
predictions and models that demonstrate improved fidelity to wet-lab 
experiments.

\begin{table}
\caption{Metric entropy ratio (ratio of clusters to entries in database) and
fractal dimension at typical search radii for four biological data sets.
NCBI's ``NR'' non-redundant protein sequence and ``NT'' non-redundant nucleotide sequence databases are from June, 2015. Protein Databank (PDB) is from July, 2015. PubChem is from October, 2013.}
\begin{tabular}{ccc}
\hline
Data set & Metric entropy ratio & Fractal dimension \\        
\hline
Nucleotide sequences (NCBI NT) & 7:1 & 1.5\\
\hline
Protein sequences (NCBI NR) & 5:1 & 1.6\\
\hline
Protein structure (PDB) & 10:1 & 2.5\\
\hline
Chemical structure (PubChem) & 11:1 & 0.2\\
\hline
\end{tabular}
\end{table}



% Bibliography
% \bibliographystyle{ACM-Reference-Format-Journals}
\bibliographystyle{abbrv}
\bibliography{main}



\end{document}
% End of v2-acmsmall-sample.tex (March 2012) - Gerry Murray, ACM


